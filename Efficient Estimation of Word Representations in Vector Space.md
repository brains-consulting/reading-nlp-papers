# Efficient Estimation of Word Representations in Vector Space 翻訳

## Abstract
我々は、巨大なデータセットから単語の連続するベクトル表現を計算する、二つの新しいモデルアーキテクチャを提案する。これらの表現の質は、単語の類似性を測ることで計測され、その結果は既存の別のニューラルネットワークを用いた技術で一番パオフォーマンスの良かったものと比較される。我々は、より小さなコンピュータコストのもとで精度における多大な改良があることを観測した。16億の単語数から高い質の単語ベクトルを得るのにかかった時間は一日にも満たなかった。さらに、これらのベクトルは統語的、意味的な類推テストにおいて、最先端のパフォーマンスを示す。

## 1 Introduction
現在のNLPシステムと技術の多くは、単語をアトミックな単位として扱う。つまり、単語の間には類似性の概念のない、ボキャブラリーのインデックスとして表現されてしまう。この選択には、いくつかの良い点もある。単純性、堅牢性があり、大量のデータでトレーニングされたシンプルなモデルは、少ないデータでトレーニングされた複雑なモデルより良いパフォーマンスを上げるということも観測されている。一例をあげる、統計的言語モデルで使われる、人気の根強いN-gramモデルがある。今日では実質的にほぼすべてのデータについてN-gramをトレーニングさせる事ができる。  
しかし、シンプルな技術には様々なタスクにおいて限界がある。例えば、話し言葉の自動認識のための関連する領域のデータ量には限界がある。そのパフォーマンスは、ほとんどの場合書き言葉が持つ潤沢なデータにより圧倒される。（たいてい数百万の単語である）。機会翻訳では、多くの言語にて、現存するコーパスはがたった数十億以下の単語しか含んでいない。よって、基礎的な技術を単純なスケールアップすることは、意義のある進歩を踏み出さず、我々はより高度な技術に集中接ざるをえなかった。  
近年の機械学習技術の進歩により、より複雑なモデルをより大きなデータを使って学習させることが可能になり、多くの場合シンプルなモデルを凌駕するようになった。おそらく、最も成功した考えは、単語の分散表現を使うことだ。たとえば、言語をベースにしたニューラルネットワークは、N-gramモデルより大幅によいパフォーマンスを示す。

### 1.1 論文のゴール
この論文の一番のゴールは、数十億から数百万もの単語のボキャブラリーを持つ膨大なデータセットから質の高い単語ベクトルを学習することができる技術を紹介することだ。我々の知る限り、既存のアーキテクチャーで数億以上の単語から、50-100ほどの比較的小さな次元の単語ベクトルを得ることに成功したものはない。  
我々は、最近提唱されたベクトル表現をの質を測る技術をつかう。類似する単語はお互いに近いだけでなく、類似性の多自由度系？(multiple degrees of similarity)を持つと想定する。これは、早くには屈折語のコンテキスト観察されたものである。例えば、名刺は複数の語尾を持ち、もし類似する単語を元のベクトル空間から探すとすると、同じような語尾の単語を探すことが可能である。  
興味深いのは、単語表現の類似性は、単なる構文規則を超えている点にある。単純な代数的操作による単語の足し引きを単語空間ですることができ、たとえは、vector(”King”) - vector(”Man”) + vector(”Woman”) とすることで得られるベクトル表現は、"Queen"という単語のベクトル表現に最も近くなる。  
この論文では、単語間の線形的規則を保持する新しいモデルアーキテクチャーを発展させることで、ベクトル操作の精度を最大化することに挑戦する。我々は構文的、意味的な規則両方を測定する新しい包括的なテストセットを考案した。そして、そのような規則性の多くが高い精度で学習されることを示す。さらに、トレーニング時間と精度がいかに単語空間の次元とトレーニングデータの量に依存するかを議論する。

### 1.2 既存の研究
単語を連続するベクトルとして表現することには長い歴史がある。ニューラルネットワーク言語モデル(NNLM)を推定する非常に人気のあったモデルは[1]にて提案された。そこでは、フィードフォワード（前もって影響を無くす仕組み）を持つ、線形射影層と非線形隠れ層を持つニューラルネットワークが、単語のベクトル表現と統計的言語モデルを学習するように使われていた。この研究は多くの人に追従された。  
他の興味深いNNLMのアーキテクチャは[13, 14]で提唱されており、そこでは、まず、単語ベクトルが単一の隠れ層を持つニューラルネットワークによって学習される。そしてその単語ベクトルはNNLMをトレーニング刷るために使われる。よって、単語ベクトルは完全なNNLMを構築することなしに学習される。この論文では、我々はこのアーキテクチャーを直接的に拡張し、ただ1単語ベクトルが単純なモデルによって学習される第一のステップに着目する。  
後ほど、この単語ベクトルは多くのNLPアプリケーションを多大に改善、簡単にするために使うことができることを示す。単語ベクトルの推定それ自体も、異なるモデルと種々のコーパスを使って行われた。それによって得られた単語ベクトルのいくつかは、将来的な研究と比較のため使うことができるようにしてある。しかし、我々の知る限り、これらのアーキテクチャーは[13]で提唱されたよりも遥かに高価なコンピュータ資源を必要とした。例外的に、対角重み行列（diagonal weight matrices）がつく割れた、ログバイリニア（log-bilinear）モデルの一種がある。[23]

## 2 モデルアーキテクチャ
連続的な単語表現を推定するために、多くの異なるタイプのモデルが提唱されてきた。有名なものだと潜在的意味解析(LSA)や潜在的ディリクレ配分法(LDA)がある。この論文では、ニューラルネットワークを用いた単語の分散表現に焦点を合わせる。それは前段で示されたように、単語の線形的規則を保持する点でLSAよりも遥かに優れたパフォーマンスをだす。LDAは大きなデータセットに置いてはより多くの高価なコンピュータ資源を必要とする。  
[18]と同じように、異なるモデルアーキテクチャを比較するため、我々は、まずモデルの計算の複雑性をモデルを完全にトレーニングするためにアクセスされる必要のあるパラメータの数として定義する。次に、我々は計算の複雑性を最小化する一方、精度を最大化するように挑戦する。

下記のすべてのモデルについて、トレーニングの複雑さは次式に比例する。  
```O = E x T x Q``` (1)  
ここで```E```はトレーニングのエポック数、```T```はトレーニングセットの中の単語数、```Q```は後ほどそれぞれのモデルにより定義される。一般的には```E =  3 - 50``` Tは10億以下となるすべてのモデルは確率的勾配降下法(stochastic gradient descent)と逆伝搬法(backpropagation)を用いてトレーニングされる。[26]

### 2.1 フィードフォワードニューラルネット言語モデル(NNLM)
確率的フィードフォワードニューラルネットワーク言語モデルは[1]で提案された。これは、インプット層、投影層、隠れ層、アウトプット層により構成される。インプット層では、```N```個の前の単語が、```1-of-V```コーディングによりエンコードされる。ここで```V```はボキャブラリーのサイズである。そのインプット層は、今度は投影層 ```P``` に投影される。```P```は ```N x D``` の次元を持ち、共有される投影行列を使う。いつでも```N```個のインプットがアクティブなので、投影層の構成は比較的簡単な操作となる。  
NNLMアーキテクチャは、投影層の値がより密集するにつれて増大する投影層と隠れ層の間の計算により複雑になる。一般的には```N``` = 10 で、投影層(P)のさいずは500から2000となるだろう。一方、隠れ層のサイズ```H```はたいてい500から1000単位となる。さらに、隠れ層はボキャブラリー中のすべての単語についての確率分散を計算するのに使われ、その結果```V```次元のアウトプット層となる。よって、それぞれのトレーニング例の計算の複雑さは  
```Q = N x D + N x N x H + H x V```  
ここで、支配的な項は ```H x V``` である。しかし、それを避けるためのいくつかの実際的な解決方法が提案されている。ソフトマックスの階層的なバージョンを使う方法[25, 23, 18]か、トレーニング中に正規化されていないモデルを使うことで正規化されたモデルを完全に避ける方法[4, 9]がある。ボキャブラリーをバイナリツリーで表現することで、評価される必要のあるアウトプット数は log2(V)にまで下げることができる。よって、大部分の複雑性は、 ```N x D x H``` の項により引き起こされることになる。  
我々のモデルでは、階層的ソフトマックスが使われ、そこではボキャブラリーはハフマンバイナリツリーによって表現される。これは、単語の頻度は、ニューラルネット言語モデルで分類を獲得するのに役に立つ[16]、という観察から導き出される。ハフマンツリーは、頻度の高い単語には短いバイナリコードを割り振ることで、評価される必要のあるアウトプットの数を削減する。バランスのとれたバイナリツリーは、log2(V)の計算量が必要である一方、、ハフマンツリーは階層的ソフトマックスをベースにしており、約log2(Unigram_perplexity(V))だけの計算量をひつ量とする。例えば、ボキャブラリーサイズが100万単語のとき、約2倍のスピードアップが得られる。計算のボトルネックは```N x D x H```にあるので、これはニューラルネットワークLMにとっては決定的なスピードアップではないが、我々は後ほど隠れ層を持たないアーキテクチャを提案するので、そうすると計算量はソフトマックス正規化の効率性にかなり依存するようになる。

### 2.2 リカレントニューラルネットワーク(RNNLM)
リカレントニューラルネットワークは、フィードフォワードNNLMのある種の制約を克服するために提案された言語モデルに基づく。文脈の長さ（モデル```N```のオーダ）を指定する必要性があり、また、理論的にはRNNは浅いニューラルネットワークよりも効率的に複雑なパターンを表現することができる。RNNモデルは、投影層を持たず、ただインプット層、隠れ層、アウトプット層を持つ。この種のモデルの特徴は、時間差のあるコネクションを通じた、隠れ層から自分自信への再帰的な行列(recurrent matrix)である。これにより、リカレントモデルはある種の短期記憶を形成し、隠れ層の状態により過去の情報が表現され、現在のインプットと過去の試行による隠れ層の状態を元にから隠れそうがアップデートされる。RNNモデルのトレーニング例の複雑性は  
```Q = H x H + H x V``` (3)  
ここで単語表現```D```は隠れ層```H```と同じ次元を持つ。先ほどと同じように、```H x V```の項は階層的ソフトマックスを用いることで```H x log2(V)```にまで効率的に削減することができる。複雑性の大部分は、```H x H```の部分になる。

### 2.3 ニューラルネットワークの並列トレーニング
膨大なデータ・セットからモデルをトレーニングするため、我々はDistBelief[6]と呼ばれる大規模な分散型フレームワークによるモデルをいくつか実装した。それには、フィードフォワードNNLMと、この論文で提案される新しいモデルが含まれている。このフレームワークによって、同じモデルの複数のレプリカを並列に実行でき、それぞれのレプリカはすべてのパラメータを保存する中央サーバを通じて、その勾配の更新を同期させる。この並列トレーニングのために、我々はAdagrad[7]と呼ばれる適応性のある学習率処理を用いてミニバッチによる非同期勾配急降下法を使った。このフレームワークでは、一般的に百以上のレプリカが使われう、それぞれがデータセンタ内の違うマシーンの多くのCPUコアを使う。

## 3 新しいログ線形モデル
このセクションで、我々は計算量の複雑さを最小化しようとする単語の分散処理を学習のための二つの新しいモデルアーキテクチャを提案する。前のセクションで得られた主な観察は、複雑性の大部分は、モデルの中の非線形的な隠れ層によるものだということであった。これはニューラルネットワークを魅力的にする部分ではあるが、我々はよりシンプルなモデルを探求することにした。それはニューラルネットワークよりは正確にデータを表現出来ないかもしれないが、よりデータを効率的にトレーニング出来るようになるものである。  
この新しいアーキテクチャは、[13, 14]で既に提案されたものに従っており、そこではニューラルネットワーク言語モデルは2ステップでトレーニングされることに成功していた。第一に、連続的な単語ベクトルがシンプルなモデルで学習され、次いでN-gramのNNLMがそれらの単語の分散表現の上でトレーニングされる。これらは後ほど、単語ベクトルの学習に集中するとかなりの量の仕事になることになるが、我々は[13]で示されたアプローチが最もシンプルなものであると考える。また、同じようなモデルはさらに以前にに提案されている。[26, 8]

### 3.1 Continuous Bag-of-Words Model
始めに提案されるアーキテクチャは、フィードフォワードNNLMに似ている。そこでは非線形隠れ層は除去され、投影層はすべての単語（投影行列ではない）に共有される。よって、すべての単語は同じポジションに投影される（それらのベクトルは平均される）。我々はこのアーキテクチャを```bag-of-words```モデルと呼ぶ。履歴上の単語の順番は到底に影響を与えないからだ。我々は、次のセクションで導入される4つの特徴と4つの履歴上の単語をインプットとするログ線形(log-linear)分類器作ることで、一番良いパフォーマンスを得ることが出来た。そこでは、トレーニングの評価は現在の（真ん中の）単語を正確に分類出来るかによる。トレーニングの複雑さは  
```Q = N x D + D x log2(V)``` (4)  
我々はこのモデルを更に推し進めて、```CBOW```を示す。それは標準的なbag-of-wordsとは違い、文脈の連続的な分散表現を使う。このモデルアーキテクチャは図の1で示される。NNLMと同じように、インプット層と投影層の重み付け行列はすべての単語で共有されることに注意して欲しい。

### 3.2 Continuous Skip-gram Model
2つ目のアーキテクチャはCBOWに似ている。しかし、文脈中の現在の単語を予想刷る代わりに、これは同じ分の中の他の単語から、単語の分類を最大化しようとする。より正確に言うと、我々は、現在の単語それぞれを連続的な投影層を持つログ線形分類器のインプットとして使い、ある範囲においてその現在の単語の前と後ろにある単語を予測する。我々は、この範囲を広くすることで、得られる単語ベクトルの質を改善出来ることを発見したが、これはまた計算の複雑性も増してしまう。より遠くにある単語は近くにある単語に比べ、たいていの場合現在の単語により関係が薄くなるので、我々は、トレーニング例の中で遠い単語のサンプリングを少なくすることで重みを小さくした。　　
このアーキテクチャのトレーニングの複雑性は下記に比例する。　　
```Q = C x (D + D x log2(V))``` (5)  
ここで```C```は単語の距離の最大値である。よって、もしC=5とすると、我々が選択するそれぞれのトレーニング単語はランダムに```< 1;C >```の範囲の数字```R```となり、現在の単語から数えて、その前に出現するR個の単語と、その後に出現するR個の単語を正解のラベルとする。これは```R x 2```回の単語分類を必要とする。現在の単語をインプットとした場合、```R + R```個の単語がアウトプットとして出てくるからである。下記の試行では、我々はC=10を使う。

# 4 結果
単語ベクトルの異なるバージョンを比較するために、既存の論文ではたいていの場合、単語とその単語に最も近い単語を示すテーブルを使い、それらのintuitivelyをはかっている。ただ、```France```という単語が```Italy```やその他の国と似ているということは簡単に言えるが、次に見るように、それらのベクトルをより複雑な類推タスクでsubjectingするるは難しい。我々は、単語の類似性にはたくさんの異なるタイプがあるとういう既存の観察を踏襲する。例えば、```big```という単語は```bigger```に似ており、同じ意味で```small```は```smaller```に似ている。他の種類の関係性の例は、```big-biggest```と```small-smallest```のペアに見ることが出来る[20]。我々は更に、同じ関連性をもつ二つの単語ペアを質問として示す。例えば、我々は次のように問うことが出来る。「```biggest```と```big```が似ているのと同じ意味で、```small```に似ている単語は何だろうか？」　　
幾分驚くべきことに、これらの質問は、単語ベクトルの単純な代数的操作を行うことで答えることが出来る。biggestがbigに似ているのと同じ意味でsmallに似ている単語は何かを探るためには、単純にベクトル ```X = vector("biggest") - vector("big") + vector("small")```を計算することができる。そして、コサイン距離を用いてXに最も近い単語を探し、それを質問の答えとして使う（我々はこの探索の間、インプットされた質問の単語を捨てる）。単語ベクトルがよく訓練されていれば、この方法で正しい答え(smallest)を探すことが可能なはずである。  
最後に、我々は、大量のデータから高い次元の単語ベクトルをトレーニングすることで、すごく繊細な単語の意味的な関係性を答えることにも使えることを発見した。例えば、街とそれが属する国との関係、フランスとパリ、ドイツとベルリンのような関係である。単語ベクトルのそのような意味的な関連性は、多くのNLPアプリケーションを改善することに使えるだろう。機械翻訳、情報抽出、質問応答システム、そして将来的に発明される種々のアプリケーションに対して使えるだろう。  

### 4.1 タスクの説明
単語ベクトルの質を測るため、我々は、5つの意味的な質問と9つの構文的質問を含む包括的なテストセットを定義した。それぞれのカテゴリーの中から2つの例をテーブルに示した。すべて含めて、8869の意味的質問と、10675の構文的質問がある。それぞれのカテゴリーにおける質問は、2つのステップで作られる。第一に、類似する単語のペアのリストが人により作られる。そして、2つの単語ペアをつなげることで、大きな質問リストが形成される。例えば、我々は68のアメリカの大きな都市と、それが属する州のリストを作り、2つのペアをランダムにピックアップして2500もの質問を作成した。

## 6 結論
この論文では、一連の構文的、意味的タスクにおいて、種々のモデルに由来するベクトル表現の質を研究した。我々は、よく使われるニューラルネットワーク（フィードフォワードとリカレント）に比べてとてもシンプルなモデルアーキテクチャをつかうことで、高い質の単語ベクトルをトレーニングすることが可能であることを確認した。計算の複雑性が遥かに少ないので、より膨大なデータセットから精度の高い高次元な単語ベクトルを計算することができる。DistBelief分散フレームワークを使うことで、CBOWとSkip-gramのモデルにおいて、一兆ものコーパス、基本的には無制限のボキャブラリーにおいてトレーニングすることが可能だろう。

### 4  Follow-Up Work
さらなる改良版は[21]ですよ